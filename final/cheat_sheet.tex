\documentclass[10pt,landscape]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage{float}
\usepackage[landscape]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,relsize}
\usepackage{color,graphicx,overpic}

% Turn off header and footer
\pagestyle{empty}
\geometry{top=.25in,left=.25in,right=.25in,bottom=.25in}

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{\z@}{3ex}{2ex}
                       {\normalfont\normalsize\bfseries\textit}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{\z@}{2ex}{0.5ex}
                          {\normalfont\small\bfseries}}
\makeatother

% Don't print section numbers
\setcounter{secnumdepth}{0}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt}

\input{math_commands.tex}
\newcommand{\ruler}{\\\rule{\columnwidth}{0.25pt}\\}

% For neural net.
\newcommand{\la}[2]{#1^{(#2)}}
\newcommand{\nbs}{n^{bs}}

% -----------------------------------------------------------------------
\begin{document}
\raggedright
\footnotesize
\begin{multicols*}{3}
% multicol parameters
% These lengths are set only within the two main columns
% \setlength{\columnseprule}{0.1pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{0pt}


\section{Matrix calculus}
$
f(\bx) = \bA\bx + \tp{\bx}\bA + \tp{\bx}\bx + \tp{\bx}\bA\bx \Rightarrow
\frac{df(\bx)}{d\bx} = \tp{\bA} + \bA + 2\bx + \bA\bx + \tp{\bA}\bx
$\\
$\bH_{i,j}=\frac{\partial^2 f}{\partial x_i \partial x_j}$;
$\nabla_{x}(a\bx) = a\bI$;
$\bJ = |\frac{\partial \bx}{\partial \by}| \Leftrightarrow \inv{\bJ} = |\frac{\partial \by}{\partial \bx}|$
% --------------------------------

\section{Perceptron}
$f(\bx) = \btheta \cdot \bx + \theta_0 = \sum_{i=1}^{d}\theta_ix_i + \theta_0$,
$
\hat{y} =
\begin{cases}
    1,  & \text{if } f(x)\geq 0\\
    -1, & \text{if } f(x) < 0
\end{cases}
$
\ruler
\textbf{Decision boundary}, a \textbf{hyperplane} in $\R^d$: $H = \{\inR{\bx}{d} : f(\bx) = 0\} = \{\inR{\bx}{d} : \btheta \cdot \bx + \theta_0 = 0\}$
\ruler
$\btheta$ is the \textbf{normal} of the hyperplane,\\
$\theta_0$ is the \textbf{offset} of the hyperplane from origin,\\
$-\frac{\theta_0}{\norm{\btheta}}$ is the \textbf{signed distance} from the origin to hyperplane.
\ruler
\textbf{Perceptron algorithm},\\
Input: $(\bx_1, y_1),\dots,(\bx_n, y_n) \in \R^{d} \times \{\pm 1\}$\\
For each training example $\bx^{(i)}$:\\
\-\hspace{0.5cm} compute $\hat y$\\
\-\hspace{0.5cm} update weights: $w_j := w_j + \Delta w_j$
$\Delta w_j = \eta (y^{(i)} - \hat y^{(i)})x_j^{(i)}$
\ruler
Given a \textbf{linearly separable data}, perceptron algorithm will take no more than $\frac{R^2}{\gamma^2}$ updates to \textbf{converge},
where $R = \max_{i}{\norm{\bx_i}}$ is the radius of the data, $\gamma = \min_{i}{\frac{y_i(\btheta\cdot\bx_i)}{\norm{\btheta}}}$ is the margin.\\
Also, $\frac{\btheta\cdot\bx}{\norm{\btheta}}$ is the signed distance from H to $\bx$ in the direction $\btheta$.
\ruler
\textbf{Gradient descent} view of perceptron, minimize margin cost function
$J(\btheta) = \sum_{i}(-y_i(\btheta\cdot\bx_i))_{+}$ with $\btheta \leftarrow \btheta - \eta\nabla J(\btheta)$
% --------------------------------

\section{Logistic Regression}
Widely used for binary classification
The Logit funtion is defined as the logarithm of odds, i.e.,
$$logit(p) = \log \frac{p}{1-p}$$
$$\inv{logit}(z) = \phi(z) = \frac{1}{1+e^{-z}}$$

Optimizing Log-likelihood function:
$$J(\bw) = \sum_{i=1}^n[-y^{(i)}\log(\phi(z^{(i)}))-(1-y^{(i)})\log(1-\phi(z^{(i)}))]$$

Weight update will be the same as Adaline training. To enable regularization, add $\frac{\lambda}{2}\sum_{j=1}^m{w_j^2}$ term (L2).
% --------------------------------
\section{Support Vector Machine}
\textbf{Hard margin SVM},\\
$\min_{\bw} \norm{\bw}^2$, such that $y^{(i)}(w_0+\bw^T\bx^{(i)}) \geq 1 (\forall i = 1,\dots,n)$\\
\textbf{Soft margin SVM (with positive slack variable $\xi$)},\\
If point $x_i$ in on the wrong side, then get penalty $\xi^{(i)}$\\
new optimization problem
$$\arg min_{w, \xi^{(i)}} \frac{1}{2}||w||^2+C(\sum_{i}{\xi^{(i)}})$$

Hinge Loss
$$\xi^{(i)}=\max(0,1-y^{(i)}(w^T\cdot x^{(i)}+w_0))$$
If the point is on the wrong side of the margin, the function's value is proportional to the distance from the margin\\
SVM natural form:

$$\arg min_{w, \xi^{(i)}} \frac{1}{2}||w||^2+C(\sum_{i}\max(0,1-y^{(i)}(w^T\cdot x^{(i)}+w_0)))$$
\ruler

\textbf{Regularization and SVMs}:
Simulated data with many features $\phi(\bx)$;
C controls trade-off between margin $1/\norm{\btheta}$ and fit to data;
Large C: focus on fit to data (small margin is ok). More overfitting.
Small C: focus on large margin, less tendency to overfit.
Overfitting increases with: less data, more features.

\textbf{Width} of the margin is $\frac{2}{\norm{\overrightarrow{\bw}}}$.
\ruler
$K(\bx_i, \bx_j) = \phi(\bx_i)\cdot\phi(\bx_j)$, K is called a kernel.\\
degree-m polynomial kernel: $K_m(\bx, \tilde{\bx}) = (1+\bx\cdot\tilde{\bx})^m$\\
RBF kernel (infinite dimention): $K_{rbf}(\bx, \tilde{\bx}) = \text{exp}(-\gamma\norm{\bx-\tilde{\bx}}^2)$
% --------------------------------
\section{Data pre-processing}
\textbf{Scaling:}\\
min-max normalization: scaling to $[0,1]$ with $x^{(i)}_{norm} = \frac{x^{(i)}-x_{min}}{x_{max}-x_{min}}$\\
Standardization with $x^{(i)}_{std} = \frac{x^{(i)}-\mu_x}{\sigma_x}$
\ruler
\textbf{Feature Selection:}\\
- Simplify the model for easier interpretation\\
- Shorten training time\\
- Avoid the curse of dimensionality\\
- reduce overfitting\\
\textbf{L1 regularization:}\\
diamond shape penalty makes optimum located on axes, which encourages sparsity (while L2 regularization leads to smaller weight)\\
\textbf{Sequential Backward Selection (SBS):}\\
1. initialize $k = d$\\
2. Determine the feature $x^- = \arg \max J(X_k-x)$ that maximizes the criterion function $J$\\
3. Remove $x^-$ from feature set\\
\-\hspace{0.5cm} $X_{k-1} = X_{k}-x^-$\\
\-\hspace{0.5cm} $k = k-1$\\
4. Terminate if k equals desired number of features otherwise go to 2.\\
\textbf{Random Forest:}\\
The random forest implemented in scikit-learn already collects the feature importance value for us
% --------------------------------
\section{Dimensionality Reduction}
Principal Component Analysis (PCA) for unsupervised data compression\\
Linear Discriminant Analysis (LDA) as a supervised dimensionality reduction technique for maximizing class separability.
\ruler
\textbf{PCA}:
Finding the directions of maximum variance (as a new subspace)\\
\textbf{Transformation Matrix}:
Construct a $d \times k$ transformation matrix $\bW$ that maps a $d$-dimensional vector $\bx$ to a $k$-dimensional vector $\bz$. Typically, $d >> k$.\\
$$\bx = [x_1,x_2,\dots,x_d]$$
$$\downarrow \bx\bW, \bW \in \R^{d \times k}$$
$$\bz = [z_1,z_2,\dots,z_k]$$
\textbf{Approach}:\\
1. Standardize $d$-dimensional dataset\\
2. Construct covariance matrix\\
3. Eigen-decompose covariance matrix\\
4. Sort eigenvalues by decreasing order\\
5. Select $k$ eigenvectors which correspond to the $k$ largest eigenvalues\\
6. Construct a projection matrix $\bW$ from $k$ eigenvectors (let eigenvector be columns of $\bW$)\\
7. Transform d-dimensional input dataset $\bX$ to obtain new $k$-dimensional feature subspace\\
\textbf{Covariance matrix $\bSigma$}:\\
$$\sigma_{jk} = \frac{1}{n}\sum_{i = 1}^n{(x^{(i)}_j-\mu_j)(x^{(i)}_k-\mu_k)}$$
Here $\mu_j$ and $\mu_k$ are the sample means of features j and k, respectively.
Symmetric: $\bSigma_{i,j} = \bSigma_{j_i}$\\
Non-negative diagonal entries: $\bSigma{i,i} \geq 0$\\
Positive semidefinite: $\forall \inR{\bv}{d}, \tp{\bv}\bSigma\bv \geq 0$
\textbf{Decompose $\bSigma$}:\\
$$\bSigma v = \lambda v$$
Explained Variance: $\frac{\lambda_j}{\sum_{j=1}^{d}{\lambda_j}}$
We can plot the Explained Variance and the accumulated sum to decide how many features to choose
\ruler
\textbf{LDA}:
LDA assumes that data is normally distributed, that the classes have identical covariance matrices, and that samples are statistically independent.\\

Compare to PCA:\\
LDA's goal is to find the feature subspaces that optimize class separability.\\
\textbf{Approach}:\\
1. Standardize the d-dimensional dataset\\
2. For each class, compute the d-dimensional mean vector\\
3. Construct the between-class scatter matrix $\bS_B$ and the within-class scatter matrix $\bS_w$\\
4. Compute the eigenvectors and corresponding eigenvalues of the matrix $\inv{\bS_w} \bS_B$\\
5. Sort the eigenvalues by decreasing order\\
6. choose the top k eigenvector to construct $\bW$\\
7. Project samples onto new subspace\\
\textbf{Mean vector}:\\
Each mean vector stores the mean feature value $\mu_m$ w.r.t the samples of class i.
$$\bm_i = \frac{1}{n_i}{\sum_{x \in D_i}^{c}{\bx_m}}$$
\textbf{Construct Scatter Matrix}:\\
$$\bS_W = \sum_{i = 1}^c{\bS_i}$$
$$\bS_i = \sum_{x \in D_i}^c{(\bx-\bm_i)(\bx-\bm_i)^T}$$
Covariance matrix is a normalized version of the scatter matrix.
$$\bS_B = \sum_{i=1}^c{n_i(\bm_i-\bm)(\bm_i-\bm)^T}$$
$\bm$ is the overall mean that is computed, including samples from all classes.
\textbf{eigendecompose}:\\
Decomposing $\inv{\bS_w} \bS_B$ and obtaining eigenvectors.
The number of linear discriminants is at most $c-1$ ($\bS_B$ is sum of rank one or less matrices)\\

Then we can construct $\bW$ and project by $\bX^{'} = \bX\bW$
% --------------------------------
% --------------------------------

\section{K-Nearest Neighbors}

Choose K and a distance metric\\

Given $\bx_q$, take majority voting among its k nearest neighbors, or take the mean of $f$ values of k nearest neighbors if real-values.
$\hat{f}(\bx_q) \leftarrow \frac{1}{k} \sum_{i=1}^{k} f(\bx_i)$
\ruler
\textbf{Distance metrics:}\\
p-norm: $\norm{\bz}_p = (\sum_{i=1}^{d}|z_i|^p)^{1/p}$
\ruler
\textbf{Pros:}\\
Memory-based approach, the model adapts as we collect new training data\\
\textbf{Cons:}\\
Computational complexity (linear growth of complexity; use KD-Tree)\\
Storage space can be a challenge (no samples discarded during training)\\
Curse of dimensionality (the feature space becomes increasingly large; use feature selection or dimensionality reduction)
\ruler
\textbf{K-d tree} for space partitioning, need backtracking to find real neighbors.
% --------------------------------

\section{Decision Tree}

Typical impurity measurements:\\
\textbf{Gini Impurity}: $I_G(t)=\sum^c_{i=1}{p(i|t)(1-p(i|t))} = 1 - \sum_{i=1}^c{p(i|t)^2}$\\
$p(i|t)$ is the proportion of the samples that belong to class i for a particular node t.\\
\textbf{Entropy}: $H = \sum_{i} - p_i\log_{2}p_i$\\
\textbf{Classification error}: $I_E=1-\max\{p(i|t)\}$\\
useful for pruning but not recommended for growing decision trees as it is less sensitive to changes in the class probability of the nodes.\\
How to split:\\
\textbf{Information Gain}: $IG(D_p,f)=I(D_P)-\frac{N_{left}}{N_p}I(D_{left})-\frac{N_{right}}{N_p}I(D_{right})$\\
Split the node at the most informative features (maximize information gain at each split)
\ruler
\textbf{AdaBoost}:\\
To iteratively train a sequence of weak learners, each focusing on the examples that were misclassified by the previous learners. This is achieved by adjusting the distribution of example weights, giving more weight to the misclassified examples in each iteration.\\
weights update: $\alpha_t = \frac{1}{2}\ln \frac{1-\epsilon_t}{\epsilon_t}$\\
distribution update: $D_{t+1}(i)=\frac{D_{t}(i)}{Z_t} \times
\begin{cases}
    e^{-\alpha_t},  & \text{if } h_t{x_i} = y_i\\
    e^{\alpha_t},   & \text{if } h_t{x_i} \neq y_i
\end{cases}
$
\ruler
\textbf{Random Forest}:
1. Draw a random bootstrap sample of size n (with replacement)\\
2. Grow a decision tree. At each node:\\
\-\hspace{0.5cm} a. Randomly select d features without replacement\\
\-\hspace{0.5cm} b. Split node using the feature that provides the best split\\
3. Repeat 1. 2. k times\\
4. Aggregate and majority voting\\

\textbf{Bootstrap Sample size}:\\
decreasing n: increases randomness and diversity, thus reducing overfitting; lower overall performance\\
increasing n: may increase the degree of overfitting\\

% --------------------------------

\section{Neural Net}
\begin{figure}[H]
    \includegraphics[width=0.8\linewidth]{output.png}
\end{figure}

% --------------------------------
\section{Evaluation \& Fine-tuning}

\textbf{Evaluation Metrics}:\\
\begin{figure}[H]
    \includegraphics[width=0.5\linewidth]{3.png}
\end{figure}

False Positive Rate: $FPR = \frac{FP}{FP+TN}$\\
True Positive Rate: $TPR = Recall = \frac{TP}{FN+TP}$\\
Precision: $PRE = \frac{TP}{TP+FP}$\\
F1 score: $F_1 = \frac{2PRE \times REC}{PRE + REC}$\\

\textbf{Metrics for Multi-class classification}:\\
$$PRE_{micro} = \frac{TP_1+\dots+TP_k}{TP_1+\dots+TP_k + FP_1 + \dots + FP_k}$$
$$PRE_{macro} = \frac{PRE_1+\dots+PRE_k}{k}$$
% --------------------------------

\section{Clustering}

Hierarchical clustering algo typically have local objectives while partitional algo have global ones.\\

$C_i = \text{argmin}_{k}\norm{\bx_i-\bmu_k}^2$\\
$\bmu_k = \frac{\sum_{C_i=k}\bx_i}{\sum{C_i=k}1}$
\ruler
1.select K points as initial centroids\\
2.repeat the following:\\
\-\hspace{0.5cm} Form K clusters by\\
\-\hspace{0.5cm} Recompute the centroids of each cluster\\
Until: The centroids don't change
\ruler
Complexity O(\#points * \#clusters * \#iterations * \#attributes)\\
May yield empty clusters: choose points or points in the cluster that contribute most to SSE. Or, update the centroids after each assignment.

\textbf{Evaluation Metrics}:\\
SSE:  $SSE = \sum_{i=1}^k \sum_{x \in C_i} {dist^2(m_i,x)}$
One easy way to reduce SSE is to increase K. A good clustering with a smaller K can have a lower SSE than a poor clustering with a higher K.\\

\textbf{Limitations}: different size of clusters;Non-globular shapes; outliers. 
\ruler
\textbf{Hierarchical clustering}: Agglomerative: bottom-up merging; Divisive: split cluster from 1 to K

\textbf{Interclass similarity}:\\
Min (non-elliptical shapes; sensitive to noise and outliers); Max (robust to noise and outliers; tend to break large clusters and biased towards globular clusters); Group Average (need to average connectivity for scalability, similar to NCC); Distance between centroids;

% --------------------------------
\section{Ensemble Learning}

When the error rate of base classifier is smaller than 0.5, then the ensemble model will have higher accuracy.

\textbf{Weighted Majority Vote Classifier}:

$$\hat y = \arg \max_i \sum_{j=1}^{m}{w_j\chi_A(C_j(x) = i)}$$

- $w_j$ is a weight associated with a base classifier $C_j$,\\
- $\hat y$ is the predicted class label of the ensemble,\\
- $\chi_A$ is an indicator function, if the condition is correct then $\chi$ returns 1, else 0\\
- $A$ is a set of unique class label
\ruler
\textbf{Bagging}:\\
The key idea behind Bagging is that by training multiple base learners on different subsets of the training data, the ensemble can capture different aspects of the underlying patterns in the data. This helps to reduce the variance of the individual base learners, leading to more stable and accurate predictions.\\


\end{multicols*}

% --------------------------------

\end{document}
